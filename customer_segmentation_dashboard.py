# -*- coding: utf-8 -*-
"""customer_segmentation_dashboard

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYpXq35eWo4Q8yV5_O4cg8KL19OXuvw_
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Data/online_retail_II.xlsx'

train_data = pd.read_excel(file_path, sheet_name= 'Year 2009-2010')
test_data = pd.read_excel(file_path, sheet_name= 'Year 2010-2011')

print(train_data.head())
print(train_data.shape)
print(train_data.info())
print(train_data.describe())

train_data = train_data.dropna(how='any')
train_data = train_data.drop_duplicates()
train_data = train_data[(train_data['Quantity']>0) & (train_data['Price']>0)]
train_data.shape

print(train_data.head())
print(train_data.shape)
print(train_data.info())
print(train_data.describe())

plt.figure(figsize=(9,6))
plt.hist(np.log1p(train_data['Quantity']),bins=20)
plt.xlabel('Quantity')
plt.ylabel('Frequency')
plt.title('Distribution of Quantity')
plt.show()

plt.figure(figsize=(9,6))
plt.hist(np.log1p(train_data['Price']),bins=20)
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Distribution of Price')
plt.show()

train_data['Total_Price'] = train_data['Quantity']*train_data['Price']

plt.figure(figsize=(9,6))
plt.hist(np.log1p(train_data['Total_Price']),bins=20)
plt.xlabel('Total_Price')
plt.ylabel('Frequency')
plt.title('Distribution of Total_Price')
plt.show()

plt.figure(figsize=(9,6))
plt.boxplot(np.log1p(train_data['Total_Price']))
plt.xlabel('Total_Price')
plt.ylabel('Frequency')
plt.title('Distribution of Total_Price')
plt.show()

plt.figure(figsize=(9,6))
plt.boxplot(np.log1p(train_data['Quantity']))
plt.xlabel('Quantity')
plt.ylabel('Frequency')
plt.title('Distribution of Quantity')
plt.show()

plt.figure(figsize=(9,6))
temp1 = train_data.groupby('Country')['Quantity'].sum().reset_index()
plt.bar(temp1['Country'],temp1['Quantity'])
plt.xlabel('Country')
plt.ylabel('Total Quantity')
plt.title('Total Quantity by Country')
plt.xticks(rotation=90)
plt.show()

#FEATURE ENGINEERING

train_data['InvoiceDate_Only'] = train_data['InvoiceDate'].dt.date
snap_date = train_data['InvoiceDate_Only'].max() + pd.Timedelta(days=1)
snap_date

RFM = train_data.groupby('Customer ID').agg(
    {
        'InvoiceDate_Only': lambda x: (snap_date - x.max()).days,
        'Invoice': 'nunique',
        'Total_Price': 'sum'
    }).reset_index()
RFM.columns = ['Customer ID', 'Recency','Frequency','Monetary']
RFM.head()
print(RFM.describe())

RFM['R_rank'] = RFM['Recency'].rank(method='max',ascending='True')
RFM['F_rank'] = RFM['Frequency'].rank(method='max', ascending='False')
RFM['M_rank'] = RFM['Monetary'].rank(method='max', ascending='False')

RFM['R_norm'] = RFM['R_rank']/RFM['R_rank'].max()
RFM['F_norm'] = RFM['F_rank']/RFM['F_rank'].max()
RFM['M_norm'] = RFM['M_rank']/RFM['M_rank'].max()

RFM['RFM_Score'] = (RFM['R_norm']*0.33)  + (RFM['F_norm']*0.33) + (RFM['M_norm']*0.34)

RFM['RFM_Score'].describe()

RFM['Customer Segment'] =  RFM['RFM_Score'].apply(lambda x: 'Top Customers' if x > 0.90 else 'High Value Customers' if x > 0.75 else 'Medium Value Customers' if x > 0.5 else 'Low Value Customers' if x > 0.32 else ' Lost Customers' )

plt.figure(figsize=(9,6))
plt.pie(RFM['Customer Segment'].value_counts(), labels=RFM['Customer Segment'].value_counts().index, autopct='%1.1f%%')
plt.title('Customer Segment Distribution')
plt.show()

RFM.head()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(RFM[['Recency','Frequency','Monetary']])

pca_model = PCA(n_components=2)
rfm_PCA = pca_model.fit_transform(rfm_scaled)

print("Explained Variance by PCA Components:", pca_model.explained_variance_ratio_)

rfm_PCA = pd.DataFrame(rfm_PCA, columns=['PCA1','PCA2'])
rfm_PCA.head()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

silhouette_scores = []
interias = []
for i in range(2,11):
  kmeans = KMeans(n_clusters=i, random_state=42)
  kmeans.fit(rfm_PCA)
  interias.append(kmeans.inertia_)
  silhouette_scores.append(silhouette_score(rfm_PCA, kmeans.labels_))
silhouette_scores

plt.figure(figsize=(9,6))
plt.plot(range(2,11), silhouette_scores, marker='o')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

#ELBOW METHOD
plt.figure(figsize=((9,6)))
plt.plot(range(2,11), interias,marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

baseline_model = KMeans(n_clusters=4, random_state=42)
baseline_model.fit(rfm_PCA)
print(silhouette_score(rfm_PCA, baseline_model.labels_))

import seaborn as sns
rfm_PCA['Cluster'] = baseline_model.labels_
plt.figure(figsize=(9,6))
sns.scatterplot(data=rfm_PCA, x='PCA1', y='PCA2', hue='Cluster', palette='tab10')

RFM['Cluster'] = baseline_model.labels_  # map this using the original RFM index

sns.scatterplot(data=RFM, x='Frequency', y='Monetary', hue='Cluster', palette='Set2')
plt.title('KMeans Clusters (Frequency vs Monetary)')
plt.xlabel('Invoice Count (Frequency)')
plt.ylabel('Invoice Price (Monetary)')
plt.show()

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
rfm_tsne = tsne.fit_transform(rfm_scaled)

# Create a DataFrame for t-SNE results
tsne_df = pd.DataFrame(rfm_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Cluster'] = baseline_model.labels_

# Corrected plot using t-SNE components
plt.figure(figsize=(7,5))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Cluster', palette='tab10')
plt.title('t-SNE Visualization of KMeans Clusters')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.show()

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

results = []

for k in [3, 4]:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(rfm_PCA)

    # Silhouette score
    sil_score = silhouette_score(rfm_PCA, labels)

    # Cluster sizes and CV
    unique, counts = np.unique(labels, return_counts=True)
    cv = np.std(counts) / np.mean(counts)

    # Save results
    results.append({
        "k": k,
        "silhouette_score": sil_score,
        "cv": cv,
        "labels": labels
    })

# Convert results to dataframe for easy viewing
results_df = pd.DataFrame(results)
print(results_df)

# Compute t-SNE embeddings once
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_PCA)

# Plot the clusters for k=3 and k=4
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
for idx, res in enumerate(results):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': res['labels'].astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    axes[idx].set_title(f'KMeans Clusters (k={res["k"]})\nSilhouette: {res["silhouette_score"]:.3f}, CV: {res["cv"]:.3f}')
    axes[idx].legend(title='Cluster')

plt.tight_layout()
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

input_layer = Input(shape=(rfm_scaled.shape[1],))

# Encoder
encoded = Dense(8, activation='relu')(input_layer)
encoded = Dense(3, activation='relu')(encoded)  # latent space

# Decoder
decoded = Dense(8, activation='relu')(encoded)
decoded = Dense(rfm_scaled.shape[1], activation='linear')(decoded)

autoencoder = Model(input_layer, decoded)
encoder = Model(input_layer, encoded)

autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)

history = autoencoder.fit(
    rfm_scaled, rfm_scaled,
    epochs=100,
    batch_size=32,
    validation_split=0.1,  # Add validation split
    verbose=1,
    callbacks=[early_stop]
)

rfm_encoded = encoder.predict(rfm_scaled)

print(rfm_encoded.shape)
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

for k in range(3, 9):
    gmm = GaussianMixture(n_components=k, random_state=42)
    labels = gmm.fit_predict(rfm_encoded)
    score = silhouette_score(rfm_encoded, labels)
    print(f"GMM Silhouette Score for k={k}: {score:.3f}")

# Store results
results = []

for k in [2, 3, 4]:
    gmm = GaussianMixture(n_components=k, random_state=42)
    labels = gmm.fit_predict(rfm_encoded)

    # Calculate silhouette score
    sil_score = silhouette_score(rfm_encoded, labels)

    # Cluster size counts
    unique, counts = np.unique(labels, return_counts=True)
    cluster_sizes = dict(zip(unique, counts))

    # Calculate Coefficient of Variation (CV)
    counts_array = np.array(list(cluster_sizes.values()))
    cv = np.std(counts_array) / np.mean(counts_array)

    # Append results
    results.append({
        "k": k,
        "silhouette_score": sil_score,
        "cluster_sizes": cluster_sizes,
        "cv": cv,
        "labels": labels
    })

    print(f"GMM k={k}: Silhouette Score = {sil_score:.3f}, Cluster Sizes = {cluster_sizes}, CV = {cv:.3f}")

tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_encoded)

# Plot clusters for each k
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for idx, res in enumerate(results):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': res['labels'].astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    axes[idx].set_title(f'GMM Clusters (k={res["k"]})\nSilhouette: {res["silhouette_score"]:.3f}, CV: {res["cv"]:.3f}')
    axes[idx].legend(title='Cluster')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE

# Define ranges of DBSCAN parameters to try
eps_values = np.arange(0.5, 5.1, 0.5)   # eps from 0.5 to 5.0 in steps of 0.5
min_samples_values = range(3, 11)       # min_samples from 3 to 10

results = []

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(rfm_encoded)

        # Ignore cases with only 1 cluster or all noise (-1)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        # Relaxing the cluster count requirement slightly for potential matches
        if n_clusters < 2: # Change condition to require at least 2 non-noise clusters
            continue

        # Silhouette score (ignoring noise)
        try:
            mask = labels != -1
            if np.sum(mask) < 2 or len(np.unique(labels[mask])) < 2: # Ensure at least 2 points in at least 2 non-noise clusters
                 sil_score = np.nan
            else:
                sil_score = silhouette_score(rfm_encoded[mask], labels[mask])
        except Exception as e: # Catch potential errors during silhouette score calculation
            print(f"Error calculating silhouette for eps={eps}, min_samples={min_samples}: {e}")
            sil_score = np.nan


        # Cluster sizes (excluding noise)
        labels_no_noise = labels[mask]
        unique, counts = np.unique(labels_no_noise, return_counts=True)
        cluster_sizes = dict(zip(unique, counts))

        # Coefficient of Variation for cluster sizes
        sizes = np.array(list(cluster_sizes.values()))
        cv = sizes.std() / sizes.mean() if sizes.mean() > 0 and len(sizes) > 1 else np.nan # Ensure more than one cluster size for CV

        results.append({
            "eps": eps,
            "min_samples": min_samples,
            "silhouette_score": sil_score,
            "cluster_sizes": cluster_sizes,
            "cv": cv,
            "labels": labels
        })

# Convert results to DataFrame for easy filtering
df_results = pd.DataFrame(results).dropna(subset=['silhouette_score'])

# Add a check if df_results is empty
if df_results.empty:
    print("No valid DBSCAN parameter combinations found based on the criteria (at least 2 non-noise clusters and valid silhouette score). Consider adjusting eps_values, min_samples_values, or the cluster count threshold.")
else:
    # Sort by silhouette score descending and then by CV ascending for balanced clusters
    df_results = df_results.sort_values(by=['silhouette_score', 'cv'], ascending=[False, True]).reset_index(drop=True)

    print("Top 10 DBSCAN hyperparameter combos:")
    print(df_results.head(10))

    # t-SNE embedding for visualization (fit once)
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    tsne_result = tsne.fit_transform(rfm_encoded)

    # Plot top 3 DBSCAN clustering results
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    # Ensure we don't try to plot more rows than exist
    num_plots = min(3, len(df_results))

    for i in range(num_plots):
        row = df_results.iloc[i]
        # Mask noise points for plot clarity
        cluster_labels = row['labels'].copy()
        cluster_labels = np.where(cluster_labels == -1, 'noise', cluster_labels.astype(str))

        df_plot = pd.DataFrame({
            'TSNE1': tsne_result[:, 0],
            'TSNE2': tsne_result[:, 1],
            'Cluster': cluster_labels
        })

        sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[i], palette='tab10', s=40, legend='full')
        axes[i].set_title(f"DBSCAN (eps={row['eps']}, min_samples={row['min_samples']})\nSilhouette: {row['silhouette_score']:.3f}, CV: {row['cv']:.3f}")
        axes[i].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc=2)

    # Hide any unused subplots
    for j in range(num_plots, 3):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering, SpectralClustering
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE

# Run t-SNE once for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_encoded)

# Store all results
all_results = []

# Agglomerative clustering sweep
linkages = ['ward', 'complete', 'average', 'single']
cluster_range = range(2, 9)

for linkage in linkages:
    for k in cluster_range:
        # Skip ward linkage if incompatible
        if linkage == 'ward' and k > len(rfm_encoded):
            continue
        model = AgglomerativeClustering(n_clusters=k, linkage=linkage)
        labels = model.fit_predict(rfm_encoded)
        sil_score = silhouette_score(rfm_encoded, labels)
        counts = np.bincount(labels)
        cv = np.std(counts) / np.mean(counts) if len(counts) > 1 else 0

        all_results.append({
            'method': 'Agglomerative',
            'linkage': linkage,
            'k': k,
            'silhouette_score': sil_score,
            'cv': cv,
            'labels': labels
        })

# Spectral clustering sweep
for k in cluster_range:
    model = SpectralClustering(n_clusters=k, random_state=42, assign_labels='kmeans')
    labels = model.fit_predict(rfm_encoded)
    sil_score = silhouette_score(rfm_encoded, labels)
    counts = np.bincount(labels)
    cv = np.std(counts) / np.mean(counts) if len(counts) > 1 else 0

    all_results.append({
        'method': 'Spectral',
        'k': k,
        'silhouette_score': sil_score,
        'cv': cv,
        'labels': labels
    })

# Print a summary table for all results
summary = []
for res in all_results:
    if res['method'] == 'Agglomerative':
        summary.append({
            'Method': res['method'],
            'Linkage': res['linkage'],
            'Clusters (k)': res['k'],
            'Silhouette Score': res['silhouette_score'],
            'Cluster Size CV': res['cv']
        })
    else:
        summary.append({
            'Method': res['method'],
            'Linkage': 'N/A',
            'Clusters (k)': res['k'],
            'Silhouette Score': res['silhouette_score'],
            'Cluster Size CV': res['cv']
        })
summary_df = pd.DataFrame(summary)
print(summary_df)

# Plot all clusterings
n_plots = len(all_results)
cols = 3
rows = n_plots // cols + (1 if n_plots % cols else 0)

fig, axes = plt.subplots(rows, cols, figsize=(cols*6, rows*5))
axes = axes.flatten()

for idx, res in enumerate(all_results):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': res['labels'].astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    title = f"{res['method']} Clusters (k={res['k']})"
    if res['method'] == 'Agglomerative':
        title += f", linkage={res['linkage']}"
    title += f"\nSilhouette: {res['silhouette_score']:.3f}, CV: {res['cv']:.3f}"
    axes[idx].set_title(title)
    axes[idx].legend(title='Cluster', loc='best')
for j in range(idx+1, len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE

# Run t-SNE once for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_encoded)

# Store results
results = []
cluster_range = range(3,5)

for k in cluster_range:
    model = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = model.fit_predict(rfm_encoded)
    sil_score = silhouette_score(rfm_encoded, labels)
    counts = np.bincount(labels)
    cv = np.std(counts) / np.mean(counts) if len(counts) > 1 else 0

    results.append({
        'k': k,
        'silhouette_score': sil_score,
        'cv': cv,
        'labels': labels
    })

# Print summary
summary_df = pd.DataFrame([{'Clusters (k)': r['k'], 'Silhouette Score': r['silhouette_score'], 'Cluster Size CV': r['cv']} for r in results])
print(summary_df)

# Plot all clusterings
fig, axes = plt.subplots(1, len(results), figsize=(5 * len(results), 5))
if len(results) == 1:
    axes = [axes]

for idx, res in enumerate(results):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': res['labels'].astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    axes[idx].set_title(f'Ward Agglomerative (k={res["k"]})\nSilhouette: {res["silhouette_score"]:.3f}, CV: {res["cv"]:.3f}')
    axes[idx].legend(title='Cluster', loc='best')

plt.tight_layout()
plt.show()

#advanced model
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Models with k=3
k = 3

# KMeans
km = KMeans(n_clusters=k, random_state=42)
am_km_labels = km.fit_predict(rfm_encoded)
km_silhouette = silhouette_score(rfm_encoded, am_km_labels)
unique, counts = np.unique(am_km_labels, return_counts=True)
km_cv = np.std(counts) / np.mean(counts)

# GMM
gmm = GaussianMixture(n_components=k, random_state=42)
am_gmm_labels = gmm.fit_predict(rfm_encoded)
gmm_silhouette = silhouette_score(rfm_encoded, am_gmm_labels)
unique, counts = np.unique(am_gmm_labels, return_counts=True)
gmm_cv = np.std(counts) / np.mean(counts)

# Agglomerative (Ward linkage)
agglo = AgglomerativeClustering(n_clusters=k, linkage='ward')
am_agglo_labels = agglo.fit_predict(rfm_encoded)
agglo_silhouette = silhouette_score(rfm_encoded, am_agglo_labels)
unique, counts = np.unique(am_agglo_labels, return_counts=True)
agglo_cv = np.std(counts) / np.mean(counts)




# Prepare t-SNE embedding for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_encoded)

# Plotting
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

models = [
    ('KMeans', am_km_labels, km_silhouette, km_cv),
    ('GMM', am_gmm_labels, gmm_silhouette, gmm_cv),
    ('Agglomerative', am_agglo_labels, agglo_silhouette, agglo_cv)
]

for idx, (name, labels, sil_score, cv) in enumerate(models):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': labels.astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    axes[idx].set_title(f'{name} Clusters (k=3)\nSilhouette: {sil_score:.3f}, CV: {cv:.3f}')
    axes[idx].legend(title='Cluster')

plt.tight_layout()
plt.show()

# Summary dataframe
summary_df = pd.DataFrame({
    'Model': ['KMeans', 'GMM', 'Agglomerative'],
    'Silhouette Score': [km_silhouette, gmm_silhouette, agglo_silhouette],
    'Cluster Size CV': [km_cv, gmm_cv, agglo_cv]
})

print(summary_df)

#base line model
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Models with k=3
k = 3

# KMeans
km = KMeans(n_clusters=k, random_state=42)
bm_km_labels = km.fit_predict(rfm_PCA)
km_silhouette = silhouette_score(rfm_PCA, bm_km_labels)
unique, counts = np.unique(bm_km_labels, return_counts=True)
km_cv = np.std(counts) / np.mean(counts)

# GMM
gmm = GaussianMixture(n_components=k, random_state=42)
bm_gmm_labels = gmm.fit_predict(rfm_PCA)
gmm_silhouette = silhouette_score(rfm_PCA, bm_gmm_labels)
unique, counts = np.unique(bm_gmm_labels, return_counts=True)
gmm_cv = np.std(counts) / np.mean(counts)

# Agglomerative (Ward linkage)
agglo = AgglomerativeClustering(n_clusters=k, linkage='ward')
bm_agglo_labels = agglo.fit_predict(rfm_PCA)
agglo_silhouette = silhouette_score(rfm_PCA, bm_agglo_labels)
unique, counts = np.unique(bm_agglo_labels, return_counts=True)
agglo_cv = np.std(counts) / np.mean(counts)


# Prepare t-SNE embedding for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
tsne_result = tsne.fit_transform(rfm_PCA)

# Plotting
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

models = [
    ('KMeans', bm_km_labels, km_silhouette, km_cv),
    ('GMM', bm_gmm_labels, gmm_silhouette, gmm_cv),
    ('Agglomerative', bm_agglo_labels, agglo_silhouette, agglo_cv)
]

for idx, (name, labels, sil_score, cv) in enumerate(models):
    df_plot = pd.DataFrame({
        'TSNE1': tsne_result[:, 0],
        'TSNE2': tsne_result[:, 1],
        'Cluster': labels.astype(str)
    })
    sns.scatterplot(data=df_plot, x='TSNE1', y='TSNE2', hue='Cluster', ax=axes[idx], palette='tab10', s=40)
    axes[idx].set_title(f'{name} Clusters (k=3)\nSilhouette: {sil_score:.3f}, CV: {cv:.3f}')
    axes[idx].legend(title='Cluster')

plt.tight_layout()
plt.show()

# Summary dataframe
summary_df = pd.DataFrame({
    'Model': ['KMeans', 'GMM', 'Agglomerative'],
    'Silhouette Score': [km_silhouette, gmm_silhouette, agglo_silhouette],
    'Cluster Size CV': [km_cv, gmm_cv, agglo_cv]
})

print(summary_df)

RFM['KM_PCA'] = bm_km_labels
RFM['GMM_PCA'] = bm_gmm_labels
RFM['AGGLO_PCA'] = bm_agglo_labels

RFM['KM_AE'] = am_km_labels
RFM['GMM_AE'] = am_gmm_labels
RFM['AGGLO_AE'] = am_agglo_labels

from scipy.stats import f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import statsmodels.api as sm
from statsmodels.formula.api import ols

def run_stat_validation(df, label_col):
    print(f"\n--- Statistical Validation for {label_col} ---")
    for metric in ['Recency', 'Frequency', 'Monetary']:
        print(f"\n{metric}:")
        groups = [group[metric].values for name, group in df.groupby(label_col)]
        f_stat, p_val = f_oneway(*groups)
        print(f"ANOVA p-value = {p_val:.4g}")
        tukey = pairwise_tukeyhsd(endog=df[metric], groups=df[label_col], alpha=0.05)
        print(tukey.summary())

for label in ['KM_PCA', 'GMM_PCA', 'AGGLO_PCA', 'KM_AE', 'GMM_AE', 'AGGLO_AE']:
    run_stat_validation(RFM, label)

#We statistically validated the quality of clusters across six segmentation models (PCA and AutoEncoder-based). Using ANOVA and Tukey HSD, we found that the KMeans model using AutoEncoder representations outperformed all others in clearly separating clusters by Recency, Frequency, and especially Monetary value (mean differences >123K, p < 0.001). This model not only satisfies statistical rigor but also identifies a high-value customer segment, making it the most suitable for business applications like targeted marketing and loyalty programs

!pip install streamlit

import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

st.title("Customer Segmentation Dashboard")

# Sidebar
st.sidebar.header("Navigation")
section = st.sidebar.radio("Go to:", ["Overview", "Cluster Distribution", "RFM Boxplots", "Cluster Profiles", "t-SNE Visualization"])

# 1. Overview
if section == "Overview":
    st.subheader("Model Selected: KMeans on AutoEncoder (KM_AE)")
    st.write("We used AutoEncoder for non-linear dimensionality reduction and KMeans for clustering.")
    st.write("Statistical validation confirms significant differences across clusters in R, F, and M values.")

# 2. Cluster Distribution
elif section == "Cluster Distribution":
    st.subheader("Customer Count by Cluster")
    cluster_counts = RFM['KM_AE'].value_counts().sort_index()
    st.bar_chart(cluster_counts)

# 3. RFM Boxplots
elif section == "RFM Boxplots":
    st.subheader("RFM Distributions by Cluster")
    for feature in ['Recency', 'Frequency', 'Monetary']:
        fig, ax = plt.subplots()
        sns.boxplot(data=RFM, x='KM_AE', y=feature, ax=ax)
        ax.set_title(f"{feature} by Cluster")
        st.pyplot(fig)

# 4. Cluster Profiles (Radar Chart or Heatmap)
elif section == "Cluster Profiles":
    st.subheader("Average RFM Values per Cluster")
    cluster_means = RFM.groupby('KM_AE')[['Recency', 'Frequency', 'Monetary']].mean().reset_index()
    st.dataframe(cluster_means)

# 5. t-SNE Visualization
elif section == "t-SNE Visualization":
    st.subheader("t-SNE Cluster Visualization")
    fig, ax = plt.subplots()
    sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='KM_AE', palette='tab10', ax=ax)
    ax.set_title("t-SNE Plot of Clusters")
    st.pyplot(fig)